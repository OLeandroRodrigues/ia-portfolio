ğŸ“ Google Reviews NLP Pipeline

This repository contains an experimental NLP pipeline to analyze and classify Google Reviews.
The project is organized into two layers:

1. Comment Classification

   ğŸ­ Sentiment analysis (positive | neutral | negative)
   ğŸ·ï¸ Topic classification (e.g., service, price, quality)
   ğŸš¨ Spam/legitimacy detection (suspect | legitimate)

2. Author Linking Experiments

    ğŸ” Detects possible same-author patterns across different accounts
    âœï¸ Based on writing fingerprint + semantic embeddings + clustering

ğŸ¯ Objectives

âœ… Train and evaluate classification models using TensorFlow.
âœ… Extract stylometry features and multilingual embeddings.
âœ… Build similarity graphs to detect repeated authorship.
âœ… Provide explainable outputs for research and audit.

ğŸ“‚ Project Structure
google-reviews-nlp/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ artifacts/                # generated outputs
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ predictions/
â”‚
â”œâ”€â”€ data/                     # datasets
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ clean-data-google-reviews.csv
â”‚
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_train_classifier.ipynb
â”‚   â”œâ”€â”€ 03_linking_experiments.ipynb
â”‚   â””â”€â”€ .ipynb_checkpoints/
â”‚
â”œâ”€â”€ src/                      # source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ heuristics_labeling.py
â”‚   â”‚   â””â”€â”€ preprocess.py
â”‚   â”‚
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ metrics_classification.py
â”‚   â”‚   â””â”€â”€ metrics_linking.py
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ embeddings_tfhub.py
â”‚   â”‚   â””â”€â”€ stylometry.py
â”‚   â”‚
â”‚   â”œâ”€â”€ link_author/
â”‚   â”‚   â”œâ”€â”€ build_similarity_graph.py
â”‚   â”‚   â””â”€â”€ cluster_dbscan.py
â”‚   â”‚
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ classifier_tf.py
â”‚       â””â”€â”€ siamese_tf.py
â”‚
â””â”€â”€ tests/                    # unit tests


âš™ï¸ Tech Stack

ğŸ Python 3.10+
ğŸ”¶ TensorFlow / Keras â€“ model training
ğŸŒ TensorFlow Hub â€“ multilingual embeddings (USE / LaBSE)
ğŸ“Š scikit-learn â€“ clustering (DBSCAN, HDBSCAN) & metrics
ğŸ“š spaCy (pt) â€“ optional preprocessing for Portuguese
ğŸ“ˆ pandas / matplotlib / seaborn â€“ analysis & visualization

ğŸ”¬ Workflow

1. Exploratory Data Analysis (01_eda.ipynb)

    -   Inspect raw reviews, distributions & anomalies
    -   Apply basic cleaning

2.  Classification (02_train_classifier.ipynb)

    -   Train sentiment / topic / spam classifiers
    -   Evaluate with accuracy, F1-score & confusion matrix

3.  Author Linking (03_linking_experiments.ipynb)

    -   Generate embeddings + stylometry features
    -   Build similarity graph & cluster comments
    -   Identify candidate same-author groups

ğŸ“Š Outputs

ğŸ“‘ Classification reports (JSON, PNG plots)
ğŸ¤– Saved TensorFlow models (.h5, .pkl)
ğŸ”— Similarity graphs of comments
ğŸ‘¥ Author cluster assignments (author_clusters.csv)

âš ï¸ Disclaimer

This project is intended for research and educational purposes only.
Author linking is probabilistic and should be interpreted as candidate same-author groups, not definitive proof.
All datasets used are either publicly available or synthetic.

ğŸš€ Next Steps

Improve weak labeling for "suspect vs legitimate" reviews.
Train a Siamese/contrastive model for authorship verification.
Add temporal & behavioral features (time, frequency).
Create an interactive dashboard for results.